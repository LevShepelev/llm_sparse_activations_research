# GPT-2 Small (124 M) Fine-Tuning on Shakespeare  
*Poetry + ğŸ¤— Transformers + MLflow*

<p align="center">
  <img src="https://raw.githubusercontent.com/akutruff/shakespeare-gpt2-assets/main/cover.jpg" width="600" alt="GPT-2 meets Shakespeare">
</p>

---

## 1 Â· Why this repo?

* **One-command training** of GPT-2-small on any folder of `.txt` files (default: Shakespeare).
* **Poetry** for reproducible env management.
* **MLflow** auto-logs âœ live curves & artifacts (loss, perplexity plot, checkpoints).
* Tiny, modular code (â‰¤ 400 LOC) thatâ€™s easy to swap for other corpora, models, or PEFT methods.

---

## 2 Â· Project Structure

gpt2_shakespeare/
â”œâ”€â”€ README.md â† you are here
â”œâ”€â”€ pyproject.toml â† Poetry deps & scripts
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ model_config.yaml â† hub model + extra tokens
â”‚ â”œâ”€â”€ train_config.yaml â† hparams, eval/log cadence
â”‚ â””â”€â”€ mlflow_config.yaml â† tracking-URI, experiment name
â”œâ”€â”€ data/
â”‚ â””â”€â”€ raw/ â† put *.txt here (multiple files OK)
â”œâ”€â”€ outputs/ â† checkpoints & artifacts (auto)
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ callbacks.py â† Perplexity + Early-Stop
â”‚ â”œâ”€â”€ config.py â† dataclass loaders
â”‚ â”œâ”€â”€ data.py â† dataset builder
â”‚ â”œâ”€â”€ model.py â† tokenizer / model loader
â”‚ â”œâ”€â”€ trainer.py â† helpers for TrainingArguments
â”‚ â”œâ”€â”€ utils.py â† misc utils
â”‚ â””â”€â”€ entry_train.py â† main training entry-point
â””â”€â”€ scripts/
â”œâ”€â”€ prepare_dataset.py â† optional cleaner
â””â”€â”€ entry_generate.py â† sample text after training

bash
Copy
Edit

---

## 3 Â· Quick Start

```bash
# 1 Â· Install Poetry (if you donâ€™t have it)
curl -sSL https://install.python-poetry.org | python3 -

# 2 Â· Clone repo & install deps
git clone https://github.com/your-org/gpt2_shakespeare.git
cd gpt2_shakespeare
poetry install --with dev   # creates virtual-env, installs GPU Torch etc.

# 3 Â· Add data
mkdir -p data/raw
cp /path/to/your/*.txt data/raw/

# 4 Â· Launch MLflow UI (optional but nice)
poetry run mlflow ui  --port 5000 &
open http://localhost:5000   # or just visit in browser

# 5 Â· Train
poetry run python -m src.entry_train \
  --model_config config/model_config.yaml \
  --train_config config/train_config.yaml \
  --data_dir data/raw \
  --pattern "*.txt"
A new MLflow run appears with live curves: train_loss, eval_loss, perplexity, plus a PNG plot when training finishes.

4 Â· Configuration Cheat-Sheet
File	Key knobs
config/model_config.yaml	model_name (hub id), add_special_tokens, resize_token_embeddings
config/train_config.yaml	num_train_epochs, learning_rate, block_size, per_device_train_batch_size, gradient_accumulation_steps, eval_steps, logging_steps, use_gradient_checkpointing, fp16 / bf16
config/mlflow_config.yaml	tracking_uri (local dir / http), experiment_name, run_name

All YAMLs are parsed into dataclasses â†’ no changes in code needed.

5 Â· Generate Text
bash
Copy
Edit
poetry run python scripts/entry_generate.py \
  --model_dir outputs/gpt2-shakespeare \
  --prompt "O, gentle night," \
  --max_new_tokens 120 \
  --temperature 0.95 \
  --top_p 0.9
entry_generate.py uses the same tokenizer; supports multiple sampling params.

