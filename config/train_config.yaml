seed: 42
output_dir: "outputs/gpt2-shakespeare"
logging_steps: 50
save_steps: 1000
save_total_limit: 3
eval_steps: 500
warmup_steps: 200
max_train_steps: null
num_train_epochs: 3
per_device_train_batch_size: 2
per_device_eval_batch_size: 2
gradient_accumulation_steps: 8
learning_rate: 5.0e-5
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-8
lr_scheduler_type: "cosine"
max_grad_norm: 1.0
fp16: true
bf16: false
block_size: 512
shuffle_buffer: 10000
eval_fraction: 0.02
push_to_hub: false
report_to: none
use_gradient_checkpointing: true
mlflow_autolog: true
use_gradient_checkpointing: true
