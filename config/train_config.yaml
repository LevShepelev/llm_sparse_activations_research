seed: 42
output_dir: "outputs/gpt2-shakespeare"
logging_steps: 10
eval_steps: 10
save_steps: 1000
evaluation_strategy: "steps"
save_total_limit: 3
warmup_steps: 200
max_train_steps: null
num_train_epochs: 20
per_device_train_batch_size: 16
per_device_eval_batch_size: 16
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
weight_decay: 0.01
adam_beta1: 0.9
adam_beta2: 0.95
adam_eps: 1.0e-8
lr_scheduler_type: "cosine"
max_grad_norm: 1.0
fp16: true
bf16: false
block_size: 512
shuffle_buffer: 10000
eval_fraction: 0.02
push_to_hub: false
report_to: none
use_gradient_checkpointing: true
mlflow_autolog: true
